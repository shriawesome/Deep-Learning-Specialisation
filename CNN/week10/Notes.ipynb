{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "## 1. Object Localisation:\n",
    "   * Model is responsible not just for object classification but also putting a bounding box around the object.\n",
    "   * Localisation refers to where the object is in the image.\n",
    "   * Localisation Understanding(considering there is single object in the image):\n",
    "   <p align='center'><img src='./imgs/ol1.png' height=\"50%\" width=\"50%\" /></p>\n",
    "    \n",
    "   * The way it differs from the classic classification task is that it has 4 more outputs added i.e. $b_{x}, b_{y}, b_{h}, b_{w}$ alonng with class labels.\n",
    "   * Also input would also include these 4 values.\n",
    "   * Understanding the output and loss:\n",
    "   <p align='center'><img src='./imgs/ol2.png' height=\"50%\" width=\"50%\" /></p>\n",
    "    \n",
    "   * where $p_c$ is whether there is an object in the image or not(1/0).\n",
    "   * For simplicity we see we are using `MSE` loss but one can use different set of loss for 3 different task, i.e for $p_c$, bounding boxes and classes\n",
    "\n",
    "## 2. Landmark Detection:\n",
    "   * Consider the following example:\n",
    "   <p align='center'><img src='./imgs/local1.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "   * Suppose if we want to detect whether the person is smiling or not or the position of the eye, we can have let's say 64 different x and y values.\n",
    "   * Each point say $l_{1x}, l_{1y}$ tells about eyes, $l_{2x}, l_{2y}$ tells about the nose and so on....(needs to be uniformed across the dataset)\n",
    "   * We can always have first value to tell us whether there is face or not? like in case of object above.\n",
    "\n",
    "## 3. Object Detection:\n",
    "   * **Car Detection Example**:\n",
    "      * Initially have a car or not model to predict whether there is car or not.\n",
    "      <p align='center'><img src='./imgs/od1.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "      * Then we implement `sliding window detection` technique to pass over a window with particular size and pass it over different regions of the image\n",
    "      \n",
    "      <p align='center'><img src='./imgs/od2.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "      * The size of the sliding window and the stride decides the computational cost of the operation\n",
    "      * If we use a very coarse stride, a very big step size, then that will reduce the number of windows you need to pass through the ConvNet, but that courser granularity may hurt performance\n",
    "      * If we use a very fine granularity or a very small stride, then the huge number of all these little regions you're passing through the ConvNet means that means there is a very high computational cost.\n",
    "\n",
    "   \n",
    "### 3.1 Convolutional Implementation of Sliding Window\n",
    "   * Need to fill it later\n",
    "\n",
    "### 3.2 Bounding Box Prediction\n",
    "   * Need to fill it later\n",
    "\n",
    "### 3.3 Intersection Over Union(IoU):\n",
    "   * Helps to evaluate an Object Detection Task\n",
    "   * Used for the mapping of **localisation to accuracy**.\n",
    "   <p align='center'><img src='./imgs/iou.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "   * The red box is the ground truth and the blue box is the prediction.\n",
    "   * Ideally we want the value to be as high as 1, but most people use 0.5 as the threshold.\n",
    "   * If you want to be stringent you can use 0.6 as the threshold.\n",
    "   * More generally, IOU is a measure of the overlap between 2 bounding boxes \n",
    "\n",
    "### 3.4 Non-max Suppression\n",
    "   * Need to fill it later\n",
    "\n",
    "### 3.5 Anchor Boxes\n",
    "   * Need to fill it later\n",
    "\n",
    "### 3.6 YOLO Algorithm\n",
    "   * Need to fill it later\n",
    "\n",
    "## 4. Image Segmentation\n",
    "### 4.1 Semantic Segmentation with U-Net\n",
    "   * Objective is to draw a careful outline around the object that is detected so that you know exactly which pixels belong to the object and which pixels don't. It tries to predict every single pixel\n",
    "   * Let's say you want to identify car, building and the load\n",
    "   <p align='center'><img src='./imgs/is1.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "   * The algorithm tries to **predict each pixel in the image to some class.**\n",
    "   * Basic architecture understanding:\n",
    "   <p><img src='./imgs/is2.png' height=\"40%\" width=\"40%\" />\n",
    "   <img src='./imgs/is3.png' height=\"55%\" width=\"55%\" /></p>\n",
    "\n",
    "   * In this case we have to restore the dimensions of the image, with each pixel refering to specific value.\n",
    "   \n",
    "### 4.2 Transpose Convolutions\n",
    "   * The goal is to take an input and blow it up to some higher dimensions than the input\n",
    "   <p align='center'><img src='./imgs/tc.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "   * The follow eg illustrates how we calculate value for different points:\n",
    "   <p align='center'><img src='./imgs/tc1.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "   * The red box value are calculated using the upper 2, green using upper 1, blue using lower 3 and so on.\n",
    "   * The values at the padding region are ignored.\n",
    "   * For the values in the intersection area the final is the total sum of all the values.\n",
    "\n",
    "### 4.3 U-net Architecture Intuition:\n",
    "   * Ideally while using conv operation in the later layers, we loose lot of `spatial information`.\n",
    "   * The middle layer may represent that looks like there's a cat roughly in the lower right hand portion of the image.\n",
    "   * The second half of this neural network uses the transpose convolution to blow the representation size up back to the size of the original input image.\n",
    "   * `Skip Connections` are implemented to copy earlier block of activations to the later block.\n",
    "   * Why skip connections?\n",
    "      *  In the final layer to decide which region is a cat, two types of information are useful:\n",
    "         * The high level, spatial, high level contextual information which it gets from this previous layer\n",
    "         * The fine grained spatial information.\n",
    "      * It allows the neural network to take this very high resolution, low level feature information where it could capture for every pixel position, how much furry stuff is there in this pixel? And used to skip connection to pause that directly to this later layer. * This way the final layer has both the lower resolution, but high level, spatial, high level contextual information, as well as the low level.\n",
    "   <p align='center'><img src='./imgs/is4.png' height=\"50%\" width=\"50%\" /></p>\n",
    "\n",
    "### 4.4 U-net Architecture\n",
    "   * Following is the descriptive understanding of U-Net architecture:\n",
    "   <p align='center'><img src='./imgs/is5.png' height=\"50%\" width=\"50%\" /></p>\n",
    "      \n",
    "   * It starts by adding more number of channels(feature maps) initially, then reduce the height and width(after max-pooling).\n",
    "   * Then after the middle part, it starts rebuildig the dimensions for the image using skip connections and transpose convolution operations.\n",
    "   * [Ronneberger et al., 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
