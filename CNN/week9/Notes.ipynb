{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Famous Networks :\n",
    " 1. **Classic networks:**\n",
    "  * LeNet - 5\n",
    "  * AlexNet\n",
    "  * VGG\n",
    " 2. **ResNet**\n",
    " \n",
    " \n",
    "# Classic Networks : \n",
    "## 1. LeNet - 5 :\n",
    " 1. Typical Architectural flow for the LeNet - 5 :\n",
    "<img src='./imgs/LeNet.png' height=\"100%\" width=\"100%\">\n",
    " 2. This was mostly used for handwritten digit classification i.e. gray scale image of digits between 0-9.\n",
    " 3. Previously, people preferred to use _Average Pooling_ in place of _Max Pooling_, also the number of parameters were very less(~60k) as compared to today(may be in millions).\n",
    " 4. We can see the general trend in the network i.e. the $n_H,n_W$ decreases as we progress deep in the network and the number of features increases(i.e. $n_C$). Even in modern implementation you would see **Conv -> Pool -> Conv -> Pool -> FC -> FC -> O/P**, also for the output today people would use softmax function(in this case with 10 o/ps).\n",
    " 5. Back then people didn't use padding much unlike now and the filters were of dimensions fxf and not fxfx$n_C$(because of computational complexity unlike now).\n",
    " 6. Reseach Paper : [LeCun et al., 1998. Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)[Useful : focus on section 2 and 3.]\n",
    " \n",
    "## 2. AlexNet :\n",
    " 1. Typical Architectural flow for AlexNet :\n",
    "<img src='./imgs/AlexNet.png' height=\"100%\" width=\"100%\">\n",
    " 2. It is very much similar to LeNet - 5, but much much bigger number of parameters(~60 millions).\n",
    " 3. It used ReLU activations.\n",
    " 4. Since during this period still the GPUs were very slow, hence it used multiple GPUs for the task.\n",
    " 5. It used something called as LRN(Local Response Normalisation), say for example we have a 13x13x256 dimensional data, it'll look for say any index data at position 1 say (5,4) and will try to normalize the entire data along 256 channel data, but researches found that it won't affect that much.\n",
    " 6. Research Paper : [Krizhevsky et al.,2012. ImageNet Classification with deep convolutional neural networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), easy to read and understand.\n",
    " \n",
    "## 3. VGG - 16\n",
    " 1. Typical Architectural design for VGG-16 :\n",
    "<img src='./imgs/VGG-16.png' height=\"100%\" width=\"100%\">\n",
    " 2. Layer Contains :\n",
    "  * CONV = 3x3 filter, s=1, padding = 'same'\n",
    "  * MAX-POOL = 2x2, s=2\n",
    " 3. Layer presents a very simple architecute with just the Conv layers and the pooling layers but at the same time consisted of very large number of parameters(~138 million).\n",
    " 4. At each Pool layer the Height and width of the volume shrinked and for each subsequent conv layers the number of channels almost doubled each time, and also the number of times convolution was performed.\n",
    " 5. Output was again 1000 different labels identified via Softmax function.\n",
    " 6. Research Paper : [Simonyan & Zisserman 2015. Very deep convolutional network for large-scale image recognition](https://arxiv.org/abs/1409.1556)\n",
    " \n",
    "# ResNets (Residual Network) : \n",
    "## Idea :\n",
    " 1. A deeper neural network may suffer from Vanishing/Exploding Gradient issues and can make the training process very difficult.\n",
    " 2. Hence, **the idea in ResNet is to use the activation say $a^{[l]}$ and skip say 1|2 layers and feed it directly after the linear activation to obtain $a^{[l+2]}$/$a^{[l+3]}$(depending upon the layers you skip) this is also termed as _'shortcut'/ 'skip connection'_**.\n",
    " 3. Architectural understanding :-\n",
    " <img src='./imgs/resnet_1.png' height=\"30%\" width=\"30%\">\n",
    " \\begin{align}\n",
    " z^{[l+1]} &= W^{[l+1]}a^{[l]}+b^{[l+1]} \\\\\n",
    " a^{[l+1]} &= g(z^{[l+1]}) \\\\\n",
    " z^{[l+2]} &= W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\\\\n",
    " a^{[l+2]} &= g(z^{[l+2]}+a^{[l]}) \\\\\n",
    " \\end{align}\n",
    " 4. Thus for a bigger NN :\n",
    " <img src='./imgs/resnet_2.png' height=\"75%\" width=\"75%\">\n",
    " As we make the NN deeper, the training error should only decrease but in reality because of exploding/vanishing gradient problem the optimisation algorithm has the harder time to optimise the algorithm.\n",
    "\n",
    "## Why it works ?\n",
    " * Say, x -> Big NN -> $a^{[l]}$\n",
    " * Also say, x -> Big NN -> $a^{[l]}$ -> ResNet -> $a^{[l+2]}$\n",
    "  * $a^{[l+2]} = g(z^{[l+2]}+a^{[l]}) ---> g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + a^{[l]}) $\n",
    "  * After say regularisation the value of say $W^{[l+2]} = 0, b^{[l+2]} = 0$ then the o/p for $g(a^{[l]})=a^{[l]}$, thus it's easy for the residual blocks to learn the **identity functions**.\n",
    "  * Hence, we can say that adding a Residual block won't hurt the performance of the function, but we also want to help the performance of the function and the fact is it is possible bcoz at times it's possible that residual block learns some important Identity functions.\n",
    "  * It is not a bad to idea to have few Residual blocks either in the middle/end of the network to help build deeper NN.\n",
    "  * Also it is important to note here that the **Dimensions for $z^{[l+2]}$ should be the same as $a^{[l]}$ to perform addition i.e. to apply padding='same'**. \n",
    "   * In case if it's not the same, consider $z^{[l+2]}$ to be of dimension (256,1) and $a^{[l]}$ to be of dimension (128,1), then consider $W_s$ of dimension (256,128) and multipy it with $a^{[l]}$ to get the final martix of dimension (256,1)\n",
    "   \n",
    "# 1 x 1 Convolution :\n",
    "## What does it do ? \n",
    " 1. Consider the following :\n",
    "<img src='./imgs/1x1_conv1.png' height=\"60%\" width=\"60%\">\n",
    "\n",
    "  * The operation is pretty straight forward and the single number say in our case 2 is multiplied each time to each element in 6x6x1 matrix and the o/p is  a matrix is same $n_H$ and $n_W$.\n",
    "  * For say input with 32 different input channels, the 1x1x32 matrix multiplies with all the channels and replace it with a single number(first multiply each 32 numbers in input to the channels of the weight matrix) and then at the end sum it all for a single number.\n",
    "  * This 1x1 conv is also termed as **Network in Network** and is useful in **Inception Network**.\n",
    "\n",
    "# Inception Layer:\n",
    "## Motivation\n",
    "  * You want to choose either a 1 x 3 filter, or 3 x 3, or 5 x 5, or do you want a pooling layer, **Why not do them all?**\n",
    "  <p><img src='./imgs/inception_1.png' height=\"40%\" width=\"40%\" /></p>\n",
    "\n",
    "  * In this case we'll have input as (28x28x192) -> (28x28x256), the results are concatenated... thus applying all the steps together and letting the model learn by itself what is best.\n",
    "  * **Problem:** \n",
    "    * Well it comes with a computational cost\n",
    "    * Let's consider an example with just 5 x 5 conv:\n",
    "    <p><img src='./imgs/inception_2.png' height=\"30%\" width=\"30%\"/><p>\n",
    "    \n",
    "    * We can see that in this we need to perform first (5x5x192) multiplications and then each number needs to be multiplied for each output i.e. 28x28x32, thus making 120M calculations.\n",
    "  * **Solution:**\n",
    "    * The calculations can be drastically reduced by using 1x1 Conv net\n",
    "    <p><img src='./imgs/inception_3.png' height=\"40%\" width=\"40%\"/><p>\n",
    "    \n",
    "    * This drastically reduces the number of calculations from 120M - 12M!\n",
    "\n",
    "## Case Studies:\n",
    "  * Let's redraw a single inception module:\n",
    "    <p><img src='./imgs/inception_4.png' height=\"40%\" width=\"40%\"/><p>\n",
    "  \n",
    "  * Regularisation effect on the inception layer:\n",
    "    * Intermediate inception blocks have FC layers along with Softmax layer\n",
    "    * It helps to ensure that the features computed. Even in the heading units, even at intermediate layers. That they're not too bad for predicting the output \n",
    "    * This appears to have a regularizing effect on the inception network and helps prevent this network from overfitting.\n",
    "    * Developed at google and named as 'gooLeNet'.\n",
    "\n",
    "# MobileNets\n",
    "## Motivation:\n",
    "  * Low computational cost at deployment\n",
    "  * Useful for mobile and embedded vision applications\n",
    "  * Key idea : Normal vs depthwise-seperable convolution\n",
    "## Comparison with Normal Convolution vs Depthwise-seperable convolution\n",
    "  <p><img src='./imgs/mbn_1.png' height=\"40%\" width=\"40%\"/><p>\n",
    "\n",
    "  * Thus we see that the total computational values are 2160\n",
    "  * **Depthwise-separable convolution**\n",
    "    * It consists of 2 steps i.e. `Depthwise Convolution` and then `Pointwise Convolution`\n",
    "    * **Depthwise Convolution**\n",
    "      * Rather than having different number of filters, it just has 1 filter with size (3 x 3) and results in $(n_{out}xn_{out}x3)$.\n",
    "      <p><img src='./imgs/mbn_2.png' height=\"40%\" width=\"40%\"/><p>\n",
    "    * **Pointwise Convolution**\n",
    "      * The output of the depthwise convolution is then convoluted with $1x1xn_{c}$ convolution with say in this case 5 different feature maps/ filters.\n",
    "      <p><img src='./imgs/mbn_3.png' height=\"40%\" width=\"40%\"/><p>\n",
    "\n",
    "  * Thus in total we performed 672 computations, which is almost 31%(672/2160) better in performance as compared to normal convolution.\n",
    "  * In the paper(Howard et al. 2017, MobileNets:Efficient CNN for Mobile Vision App), they claim the performance improvement is $\\frac{1}{n^{`}_{c}}+\\frac{1}{f^{2}}$.\n",
    "\n",
    "## MobileNets V2:\n",
    "  * In addition to the depthwise and pointwise convolution, v2 adds in the expansion block and it renames the pointwise convolution -> projection, with the addition of residual connections.\n",
    "  <p><img src='./imgs/mbn_4.png' height=\"40%\" width=\"40%\"/><p>\n",
    "  \n",
    "  * The `expansion` block enables the addition of more number of channels which leads to **learning of richer functions**.\n",
    "  * The  `projection` block reduces the amount of memory needed to store the values.\n",
    "  * Thus allow your neural network to learn richer and more complex functions, while also keeping the amounts of memory that is the size of the activations you need to pass from layer to layer, relatively small.\n",
    "\n",
    "# Practical implementation tips\n",
    "## Transfer Learning:\n",
    "  * At times we might not have enough data to train the model from scratch, in such scenarios it's a good idea to reuse weights obtained by someone else from the community after vigrous work.\n",
    "  * Depending upon the amount of data we have for traning, we can decide how many layers in the model we can set `trainable` to false/ true.\n",
    "  <p><img src='./imgs/tl.png' height=\"40%\" width=\"40%\"/><p>\n",
    "\n",
    "  * In first case, when we have very less data to train, we can set all the trainable parameters to false and just train the softmax layer parameters.\n",
    "  * In second case, when we have relatively large dataset for training, we can set some intial layer's trainable parameters to false and later layers to be true.\n",
    "\n",
    "## Data Augmentation\n",
    "  * One of the techniques used for improving the performance of the task.\n",
    "  * Different techniques encompass\n",
    "    * Mirroring over the vertical axis\n",
    "    * Random Cropping(works in pracitcal world)\n",
    "    * Rotation(used less, perhaps because of complexity)\n",
    "    * Shearing(\")\n",
    "    * Local warping(\")\n",
    "    * Color Shifting(adding different values to the RGB channel)[Motivation is driven from the fact that lighting might be different] | One principle algorithm is PCA(AlexNet Paper) Color augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
