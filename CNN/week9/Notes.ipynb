{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Famous Networks :\n",
    " 1. **Classic networks:**\n",
    "  * LeNet - 5\n",
    "  * AlexNet\n",
    "  * VGG\n",
    " 2. **ResNet**\n",
    " \n",
    " \n",
    "# Classic Networks : \n",
    "## 1. LeNet - 5 :\n",
    " 1. Typical Architectural flow for the LeNet - 5 :\n",
    "<img src='./imgs/LeNet.png' height=\"100%\" width=\"100%\">\n",
    " 2. This was mostly used for handwritten digit classification i.e. gray scale image of digits between 0-9.\n",
    " 3. Previously, people preferred to use _Average Pooling_ in place of _Max Pooling_, also the number of parameters were very less(~60k) as compared to today(may be in millions).\n",
    " 4. We can see the general trend in the network i.e. the $n_H,n_W$ decreases as we progress deep in the network and the number of features increases(i.e. $n_C$). Even in modern implementation you would see **Conv -> Pool -> Conv -> Pool -> FC -> FC -> O/P**, also for the output today people would use softmax function(in this case with 10 o/ps).\n",
    " 5. Back then people didn't use padding much unlike now and the filters were of dimensions fxf and not fxfx$n_C$(because of computational complexity unlike now).\n",
    " 6. Reseach Paper : [LeCun et al., 1998. Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)[Useful : focus on section 2 and 3.]\n",
    " \n",
    "## 2. AlexNet :\n",
    " 1. Typical Architectural flow for AlexNet :\n",
    "<img src='./imgs/AlexNet.png' height=\"100%\" width=\"100%\">\n",
    " 2. It is very much similar to LeNet - 5, but much much bigger number of parameters(~60 millions).\n",
    " 3. It used ReLU activations.\n",
    " 4. Since during this period still the GPUs were very slow, hence it used multiple GPUs for the task.\n",
    " 5. It used something called as LRN(Local Response Normalisation), say for example we have a 13x13x256 dimensional data, it'll look for say any index data at position 1 say (5,4) and will try to normalize the entire data along 256 channel data, but researches found that it won't affect that much.\n",
    " 6. Research Paper : [Krizhevsky et al.,2012. ImageNet Classification with deep convolutional neural networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), easy to read and understand.\n",
    " \n",
    "## 3. VGG - 16\n",
    " 1. Typical Architectural design for VGG-16 :\n",
    "<img src='./imgs/VGG-16.png' height=\"100%\" width=\"100%\">\n",
    " 2. Layer Contains :\n",
    "  * CONV = 3X3 filter, s=1, padding = 'same'\n",
    "  * MAX-POOL = 2X2, s=2\n",
    " 3. Layer presents a very simple architecute with just the Conv layers and the pooling layers but at the same time consisted of very large number of parameters(~138 million).\n",
    " 4. At each Pool layer the Height and width of the volume shrinked and for each subsequent conv layers the numbr of channels almost doubled each time, and also the number of times convolution was performed.\n",
    " 5. Output was again 1000 different labels identified via Softmax function.\n",
    " 6. Research Paper : [Simonyan & Zisserman 2015. Very deep convolutional network for large-scale image recognition](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
