{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Famous Networks :\n",
    " 1. **Classic networks:**\n",
    "  * LeNet - 5\n",
    "  * AlexNet\n",
    "  * VGG\n",
    " 2. **ResNet**\n",
    " \n",
    " \n",
    "# Classic Networks : \n",
    "## 1. LeNet - 5 :\n",
    " 1. Typical Architectural flow for the LeNet - 5 :\n",
    "<img src='./imgs/LeNet.png' height=\"100%\" width=\"100%\">\n",
    " 2. This was mostly used for handwritten digit classification i.e. gray scale image of digits between 0-9.\n",
    " 3. Previously, people preferred to use _Average Pooling_ in place of _Max Pooling_, also the number of parameters were very less(~60k) as compared to today(may be in millions).\n",
    " 4. We can see the general trend in the network i.e. the $n_H,n_W$ decreases as we progress deep in the network and the number of features increases(i.e. $n_C$). Even in modern implementation you would see **Conv -> Pool -> Conv -> Pool -> FC -> FC -> O/P**, also for the output today people would use softmax function(in this case with 10 o/ps).\n",
    " 5. Back then people didn't use padding much unlike now and the filters were of dimensions fxf and not fxfx$n_C$(because of computational complexity unlike now).\n",
    " 6. Reseach Paper : [LeCun et al., 1998. Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)[Useful : focus on section 2 and 3.]\n",
    " \n",
    "## 2. AlexNet :\n",
    " 1. Typical Architectural flow for AlexNet :\n",
    "<img src='./imgs/AlexNet.png' height=\"100%\" width=\"100%\">\n",
    " 2. It is very much similar to LeNet - 5, but much much bigger number of parameters(~60 millions).\n",
    " 3. It used ReLU activations.\n",
    " 4. Since during this period still the GPUs were very slow, hence it used multiple GPUs for the task.\n",
    " 5. It used something called as LRN(Local Response Normalisation), say for example we have a 13x13x256 dimensional data, it'll look for say any index data at position 1 say (5,4) and will try to normalize the entire data along 256 channel data, but researches found that it won't affect that much.\n",
    " 6. Research Paper : [Krizhevsky et al.,2012. ImageNet Classification with deep convolutional neural networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), easy to read and understand.\n",
    " \n",
    "## 3. VGG - 16\n",
    " 1. Typical Architectural design for VGG-16 :\n",
    "<img src='./imgs/VGG-16.png' height=\"100%\" width=\"100%\">\n",
    " 2. Layer Contains :\n",
    "  * CONV = 3x3 filter, s=1, padding = 'same'\n",
    "  * MAX-POOL = 2x2, s=2\n",
    " 3. Layer presents a very simple architecute with just the Conv layers and the pooling layers but at the same time consisted of very large number of parameters(~138 million).\n",
    " 4. At each Pool layer the Height and width of the volume shrinked and for each subsequent conv layers the number of channels almost doubled each time, and also the number of times convolution was performed.\n",
    " 5. Output was again 1000 different labels identified via Softmax function.\n",
    " 6. Research Paper : [Simonyan & Zisserman 2015. Very deep convolutional network for large-scale image recognition](https://arxiv.org/abs/1409.1556)\n",
    " \n",
    "# ResNets (Residual Network) : \n",
    "## Idea :\n",
    " 1. A deeper neural network may suffer from Vanishing/Exploding Gradient issues and can make the training process very difficult.\n",
    " 2. Hence, **the idea in ResNet is to use the activation say $a^{[l]}$ and skip say 1|2 layers and feed it directly after the linear activation to obtain $a^{[l+2]}$/$a^{[l+3]}$(depending upon the layers you skip) this is also termed as _'shortcut'/ 'skip connection'_**.\n",
    " 3. Architectural understanding :-\n",
    " <img src='./imgs/resnet_1.png' height=\"30%\" width=\"30%\">\n",
    " \\begin{align}\n",
    " z^{[l+1]} &= W^{[l+1]}a^{[l]}+b^{[l+1]} \\\\\n",
    " a^{[l+1]} &= g(z^{[l+1]}) \\\\\n",
    " z^{[l+2]} &= W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\\\\n",
    " a^{[l+2]} &= g(z^{[l+2]}+a^{[l]}) \\\\\n",
    " \\end{align}\n",
    " 4. Thus for a bigger NN :\n",
    " <img src='./imgs/resnet_2.png' height=\"75%\" width=\"75%\">\n",
    " As we make the NN deeper, the training error should only decrease but in reality because of exploding/vanishing gradient problem the optimisation algorithm has the harder time to optimise the algorithm.\n",
    "\n",
    "## Why it works ?\n",
    " * Say, x -> Big NN -> $a^{[l]}$\n",
    " * Also say, x -> Big NN -> $a^{[l]}$ -> ResNet -> $a^{[l+2]}$\n",
    "  * $a^{[l+2]} = g(z^{[l+2]}+a^{[l]}) ---> g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + a^{[l]}) $\n",
    "  * After say regularisation the value of say $W^{[l+2]} = 0, b^{[l+2]} = 0$ then the o/p for $g(a^{[l]})=a^{[l]}$, thus it's easy for the residual blocks to learn the **identity functions**.\n",
    "  * Hence, we can say that adding a Residual block won't hurt the performance of the function, but we also want to help the performance of the function and the fact is it is possible bcoz at times it's possible that residual block learns some important Identity functions.\n",
    "  * It is not a bad to idea to have few Residual blocks either in the middle/end of the network to help build deeper NN.\n",
    "  * Also it is important to note here that the **Dimensions for $z^{[l+2]}$ should be the same as $a^{[l]}$ to perform addition i.e. to apply padding='same'**. \n",
    "   * In case if it's not the same, consider $z^{[l+2]}$ to be of dimension (256,1) and $a^{[l]}$ to be of dimension (128,1), then consider $W_s$ of dimension (256,128) and multipy it with $a^{[l]}$ to get the final martix of dimension (256,1)\n",
    "   \n",
    "# 1 x 1 Convolution :\n",
    "## What does it do ? \n",
    " 1. Consider the following :\n",
    "<img src='./imgs/1x1_conv1.png' height=\"60%\" width=\"60%\">\n",
    "\n",
    "  * The operation is pretty straight forward and the single number say in our case 2 is multiplied each time to each element in 6x6x1 matrix and the o/p is  a matrix is same $n_H$ and $n_W$.\n",
    "  * For say input with 32 different input channels, the 1x1x32 matrix multiplies with all the channels and replace it with a single number(first multiply each 32 numbers in input to the channels of the weight matrix) and then at the end sum it all for a single number.\n",
    "  * This 1x1 conv is also termed as **Network in Network** and is useful in **Inception Network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
